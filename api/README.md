# Member 4: FastAPI Service Layer

REST API for semantic search over arXiv research papers using Milvus vector database.

## Overview

Member 4's responsibility is to:

1. Build production-ready FastAPI service
2. Expose semantic search through REST endpoints
3. Handle query encoding and Milvus integration
4. Provide health checks and monitoring
5. Generate interactive API documentation

## Directory Structure

```
api/
‚îú‚îÄ‚îÄ main.py                    # FastAPI application
‚îú‚îÄ‚îÄ models.py                  # Pydantic request/response models
‚îú‚îÄ‚îÄ config.py                  # Configuration (Milvus, model, API settings)
‚îú‚îÄ‚îÄ search_service.py          # Search logic (encode + Milvus search)
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile                 # Container for API service
‚îú‚îÄ‚îÄ docker-compose.yml         # Full stack deployment
‚îî‚îÄ‚îÄ README.md                  # This file
```

## Prerequisites

- Completed Member 3 work (Milvus cluster running with 510K indexed vectors)
- Python 3.11+
- Docker and Docker Compose (optional for containerized deployment)

## Quick Start

### 1. Install Dependencies

```bash
cd api
pip install -r requirements.txt
```

Dependencies include:

- `fastapi` - Web framework
- `uvicorn` - ASGI server
- `pymilvus` - Milvus client (from Member 3)
- `sentence-transformers` - Query encoding (from Member 2)
- `pydantic` - Data validation

### 2. Ensure Milvus is Running

Make sure Member 3's Milvus cluster is running:

```bash
cd ../milvus
docker compose ps
```

All services should be "Up" (etcd, minio, milvus-standalone, attu).

### 3. Start API Service

**Option A: Direct run (development)**

```bash
# From api/ directory
python main.py
```

Or with uvicorn:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Option B: Docker container**

```bash
# Build and run
docker build -t semantic-search-api .
docker run -p 8000:8000 \
  -e MILVUS_HOST=localhost \
  -e MILVUS_PORT=19530 \
  semantic-search-api
```

**Option C: Full stack with docker-compose**

```bash
# Start entire stack (Milvus + API)
docker compose up -d
```

### 4. Verify API is Running

```bash
curl http://localhost:8000/health
```

Expected response:

```json
{
  "status": "healthy",
  "milvus_connected": true,
  "collection_loaded": true,
  "total_entities": 510203,
  "model_loaded": true,
  "api_version": "1.0.0",
  "timestamp": "2025-12-05T01:00:00Z"
}
```

## API Endpoints

### üîç POST /search

Perform semantic search on research papers.

**Request:**

```bash
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "neural networks for image classification",
    "top_k": 5,
    "min_score": 0.6,
    "categories": ["cs.CV", "cs.LG"]
  }'
```

**Response:**

```json
{
  "query": "neural networks for image classification",
  "results": [
    {
      "paper_id": "200303253",
      "title": "Introduction to deep learning",
      "abstract": "Deep Learning has made a major impact on data science...",
      "categories": "cs.LG",
      "score": 0.6478,
      "text_length": 5234,
      "has_full_data": true
    }
  ],
  "total_results": 5,
  "latency_ms": 218.5,
  "timestamp": "2025-12-05T01:00:00Z"
}
```

**Parameters:**

- `query` (required): Natural language search query (3-500 chars)
- `top_k` (optional): Number of results (1-100, default: 10)
- `min_score` (optional): Minimum similarity score threshold (0.0-1.0)
- `categories` (optional): Filter by arXiv categories (e.g., ["cs.LG", "cs.AI"])

### üíö GET /health

Health check endpoint.

**Request:**

```bash
curl http://localhost:8000/health
```

**Response:**

```json
{
  "status": "healthy",
  "milvus_connected": true,
  "collection_loaded": true,
  "total_entities": 510203,
  "model_loaded": true,
  "api_version": "1.0.0",
  "timestamp": "2025-12-05T01:00:00Z"
}
```

### üìä GET /stats

Collection statistics.

**Request:**

```bash
curl http://localhost:8000/stats
```

**Response:**

```json
{
  "collection_name": "arxiv_papers",
  "total_entities": 510203,
  "index_type": "IVF_FLAT",
  "vector_dim": 384,
  "metric_type": "IP",
  "model_name": "sentence-transformers/all-MiniLM-L6-v2"
}
```

### üìñ GET /docs

Interactive API documentation (Swagger UI).

Open in browser: http://localhost:8000/docs

Features:

- Full API documentation
- Try out requests interactively
- See request/response schemas
- Example payloads

### üìö GET /redoc

Alternative API documentation (ReDoc).

Open in browser: http://localhost:8000/redoc

## Usage Examples

### Python Client

```python
import requests

# Search for papers
response = requests.post(
    "http://localhost:8000/search",
    json={
        "query": "deep learning for natural language processing",
        "top_k": 10,
        "min_score": 0.5
    }
)

results = response.json()
print(f"Found {results['total_results']} papers in {results['latency_ms']:.2f}ms")

for paper in results['results']:
    print(f"\n{paper['score']:.4f} - {paper['title']}")
    print(f"  Categories: {paper['categories']}")
    print(f"  Abstract: {paper['abstract'][:200]}...")
```

### JavaScript/Node.js

```javascript
const fetch = require("node-fetch");

async function search(query) {
  const response = await fetch("http://localhost:8000/search", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      query: query,
      top_k: 5,
      min_score: 0.6,
    }),
  });

  const results = await response.json();
  console.log(`Found ${results.total_results} papers`);
  return results;
}

search("quantum computing algorithms").then(console.log);
```

### cURL Examples

**Simple search:**

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "reinforcement learning robotics", "top_k": 3}'
```

**With score filtering:**

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "graph neural networks",
    "top_k": 5,
    "min_score": 0.7
  }'
```

**With category filtering:**

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "transformers for NLP",
    "top_k": 10,
    "categories": ["cs.CL", "cs.LG"]
  }'
```

## Configuration

Configuration is in `config.py`. Key settings:

```python
# Milvus Connection
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
MILVUS_COLLECTION = "arxiv_papers"

# Model Configuration
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
VECTOR_DIM = 384

# Search Parameters
DEFAULT_TOP_K = 10
MAX_TOP_K = 100
MIN_TOP_K = 1

# CORS Origins
CORS_ORIGINS = [
    "http://localhost:3000",
    "http://localhost:5173",
    "http://localhost:8080",
]
```

Environment variables (optional):

- `MILVUS_HOST` - Milvus server host (default: localhost)
- `MILVUS_PORT` - Milvus server port (default: 19530)
- `LOG_LEVEL` - Logging level (default: INFO)

## Performance

### Expected Metrics

| Metric                  | Target     | Typical        |
| ----------------------- | ---------- | -------------- |
| API Response Time       | <250ms     | 180-220ms      |
| Model Loading (startup) | -          | ~2-3s          |
| Query Encoding          | -          | ~10-30ms       |
| Milvus Search           | -          | ~150-200ms     |
| Throughput              | >100 req/s | ~120-150 req/s |

### Performance Tips

1. **Model warmup** - First query is slower due to model loading (~2s). Subsequent queries are fast.

2. **Concurrent requests** - API supports 100+ concurrent connections.

3. **Score filtering** - Using `min_score` may require fetching more results from Milvus (automatically handled).

4. **Category filtering** - Reduces search space but adds filter overhead.

5. **Batch encoding** - For multiple queries, encode in batches (not yet implemented).

## Testing

### Manual Testing

1. **Health check:**

```bash
curl http://localhost:8000/health
```

2. **Basic search:**

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "machine learning", "top_k": 3}'
```

3. **Interactive docs:**
   Open http://localhost:8000/docs in browser and try queries.

### Validation Against Member 3's Test Queries

Use the same queries from Member 3's `test_queries.py`:

```bash
# Query 1
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "neural networks deep learning", "top_k": 3}' | jq

# Query 2
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "quantum computing algorithms", "top_k": 3}' | jq

# Query 3
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "natural language processing transformers", "top_k": 3}' | jq
```

Expected: Results should match Member 3's test output (same papers, same scores).

## Error Handling

The API provides detailed error responses:

**400 Bad Request** - Invalid input

```json
{
  "error": "ValidationError",
  "message": "Query must be at least 3 characters",
  "timestamp": "2025-12-05T01:00:00Z"
}
```

**503 Service Unavailable** - Milvus connection issue

```json
{
  "error": "ServiceUnavailable",
  "message": "Search failed: Milvus connection lost",
  "timestamp": "2025-12-05T01:00:00Z"
}
```

**500 Internal Server Error** - Unexpected error

```json
{
  "error": "InternalServerError",
  "message": "An unexpected error occurred",
  "timestamp": "2025-12-05T01:00:00Z"
}
```

## Monitoring

### Logs

Application logs to stdout with structured format:

```
2025-12-05 01:00:00,123 - api.main - INFO - Starting API service...
2025-12-05 01:00:02,456 - api.search_service - INFO - ‚úì Model loaded successfully
2025-12-05 01:00:03,789 - api.search_service - INFO - ‚úì Connected to Milvus
2025-12-05 01:00:03,790 - api.main - INFO - ‚úì API service ready
2025-12-05 01:00:10,123 - api.main - INFO - Search: query='neural networks...' results=10 latency=218.45ms
```

### Health Monitoring

**Kubernetes/Docker:**

- Health endpoint: `/health`
- Liveness probe: Check `status == "healthy"`
- Readiness probe: Check `milvus_connected == true`

**Uptime monitoring:**

```bash
# Check health every 30 seconds
watch -n 30 'curl -s http://localhost:8000/health | jq'
```

## Troubleshooting

### Issue: API won't start

**Check Milvus is running:**

```bash
cd ../milvus
docker compose ps
```

**Check logs:**

```bash
# If running with Docker
docker logs semantic-search-api

# If running directly
# Check terminal output
```

### Issue: "Model not loaded" error

The model downloads on first run (~133 MB). Ensure internet connection and wait for download to complete.

**Check model cache:**

```bash
ls -lh ~/.cache/torch/sentence_transformers/
```

### Issue: Slow first query

First query includes model loading overhead (~2-3s). This is normal. Subsequent queries are fast (<250ms).

**Warmup on startup:**
The service automatically warms up the model during initialization.

### Issue: "Milvus connection failed"

**Verify Milvus is accessible:**

```bash
curl http://localhost:19530
# or
telnet localhost 19530
```

**Check Milvus logs:**

```bash
cd ../milvus
docker compose logs milvus-standalone
```

### Issue: CORS errors

Add your frontend origin to `CORS_ORIGINS` in `config.py`:

```python
CORS_ORIGINS = [
    "http://localhost:3000",
    "http://your-frontend:8080",
]
```

## Deployment

### Production Deployment

**With Gunicorn (production server):**

```bash
gunicorn main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 60
```

**With Docker:**

```bash
docker build -t semantic-search-api:latest .
docker run -d \
  --name api \
  -p 8000:8000 \
  -e MILVUS_HOST=milvus-host \
  -e MILVUS_PORT=19530 \
  --restart unless-stopped \
  semantic-search-api:latest
```

**Environment variables for production:**

- Set `MILVUS_HOST` to your Milvus server hostname
- Set `LOG_LEVEL=WARNING` for production
- Configure `CORS_ORIGINS` for your frontend domains

## Integration Points

### For Member 5 (Monitoring/Benchmarking)

**Endpoints for monitoring:**

- `GET /health` - Health checks and uptime
- `GET /stats` - Collection statistics
- Search latency logged for each request

**Metrics to track:**

- API response time (latency_ms in response)
- Request rate (from logs)
- Error rate (4xx/5xx responses)
- Milvus connection status

**Load testing:**

```bash
# Apache Bench
ab -n 1000 -c 10 -p query.json -T application/json http://localhost:8000/search

# Or use wrk, hey, locust, etc.
```

### For Frontend Integration

**CORS is enabled** for common development ports (3000, 5173, 8080).

**Example fetch:**

```javascript
fetch("http://localhost:8000/search", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    query: userInput,
    top_k: 10,
  }),
})
  .then((res) => res.json())
  .then((data) => displayResults(data.results));
```

## Next Steps (Member 5)

After Member 4 completes:

- API service running on port 8000
- All endpoints tested and functional
- Integration with Milvus validated

Member 5 will:

- Set up monitoring (Prometheus + Grafana)
- Create performance benchmarks
- Load testing and optimization
- Dashboard for metrics visualization

## Deliverables Checklist

- [x] FastAPI application with search endpoint
- [x] Pydantic models for request/response validation
- [x] SentenceTransformer integration for query encoding
- [x] Milvus search service integration
- [x] Health check and stats endpoints
- [x] Auto-generated API documentation (Swagger/ReDoc)
- [x] Error handling and logging
- [x] CORS configuration
- [x] Docker containerization
- [x] Comprehensive README with examples

---

**Estimated Completion Time:** 2-3 hours  
**Status:** Ready for implementation  
**Date:** December 5, 2025
